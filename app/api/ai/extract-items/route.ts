import { NextRequest, NextResponse } from 'next/server';
import Replicate from 'replicate';
import OpenAI from 'openai';
import { createClient } from '@supabase/supabase-js';

// Initialize clients
const replicate = new Replicate({
  auth: process.env.REPLICATE_API_TOKEN,
});

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!
);

// Check if Gemini 3 Pro is available
const useGemini = !!process.env.REPLICATE_API_TOKEN;

// Configuration pour le chunking
const MAX_LINES_PER_CHUNK = 100; // Nombre max de lignes par chunk
const MAX_CHARS_PER_CHUNK = 12000; // Nombre max de caract√®res par chunk

interface ExtractedItem {
  name: string;
  description: string | null;
  category: string;
  quantity: number | null;
  unit: string | null;
  specs: Record<string, any>;
}

interface CategorySuggestion {
  category: string;
  missingItems: Array<{
    name: string;
    reason: string;
  }>;
}

interface ExtractionResult {
  items: ExtractedItem[];
  categories: string[];
  suggestions: CategorySuggestion[];
  statistics: {
    totalItems: number;
    itemsWithQuantity: number;
    categoriesCount: number;
  };
}

interface ChunkResult {
  items: ExtractedItem[];
  categories: string[];
}

/**
 * Divise le contenu du fichier en chunks pour traitement
 */
function splitIntoChunks(fileContent: string): string[] {
  const lines = fileContent.split('\n');
  const headerLine = lines[0]; // Garder l'en-t√™te pour chaque chunk
  const dataLines = lines.slice(1).filter(line => line.trim());
  
  const chunks: string[] = [];
  let currentChunk: string[] = [headerLine];
  let currentChunkSize = headerLine.length;
  
  for (const line of dataLines) {
    // V√©rifier si on d√©passe les limites
    const wouldExceedLines = currentChunk.length >= MAX_LINES_PER_CHUNK;
    const wouldExceedChars = currentChunkSize + line.length > MAX_CHARS_PER_CHUNK;
    
    if (wouldExceedLines || wouldExceedChars) {
      // Sauvegarder le chunk actuel et en commencer un nouveau
      if (currentChunk.length > 1) { // Plus que juste l'en-t√™te
        chunks.push(currentChunk.join('\n'));
      }
      currentChunk = [headerLine, line];
      currentChunkSize = headerLine.length + line.length;
    } else {
      currentChunk.push(line);
      currentChunkSize += line.length;
    }
  }
  
  // Ajouter le dernier chunk
  if (currentChunk.length > 1) {
    chunks.push(currentChunk.join('\n'));
  }
  
  return chunks;
}

/**
 * Fusionne les r√©sultats de plusieurs chunks
 */
function mergeChunkResults(results: ChunkResult[]): { items: ExtractedItem[]; categories: string[] } {
  const allItems: ExtractedItem[] = [];
  const allCategories = new Set<string>();
  
  for (const result of results) {
    allItems.push(...result.items);
    result.categories.forEach(cat => allCategories.add(cat));
  }
  
  // D√©dupliquer les items par nom (garder le premier)
  const uniqueItems = allItems.reduce((acc, item) => {
    const exists = acc.find(i => i.name.toLowerCase() === item.name.toLowerCase());
    if (!exists) {
      acc.push(item);
    }
    return acc;
  }, [] as ExtractedItem[]);
  
  return {
    items: uniqueItems,
    categories: Array.from(allCategories).sort(),
  };
}

export async function POST(request: NextRequest) {
  try {
    const { 
      projectId, 
      fileContent, 
      fileName, 
      sectorName, 
      customSectorName 
    } = await request.json();

    if (!projectId || !fileContent) {
      return NextResponse.json(
        { error: 'Project ID and file content are required' },
        { status: 400 }
      );
    }

    // Determine sector context
    const sector = customSectorName || sectorName || 'g√©n√©ral';

    // Diviser le fichier en chunks si n√©cessaire
    const chunks = splitIntoChunks(fileContent);
    const totalChunks = chunks.length;
    
    console.log('üöÄ Starting intelligent extraction...', {
      projectId,
      fileName,
      sector,
      contentLength: fileContent.length,
      totalChunks,
      linesCount: fileContent.split('\n').length
    });

    const modelUsed = useGemini ? 'gemini-3-pro' : 'gpt-4o';
    const chunkResults: ChunkResult[] = [];

    // Traiter chaque chunk
    for (let i = 0; i < chunks.length; i++) {
      const chunk = chunks[i];
      const chunkNumber = i + 1;
      
      console.log(`üì¶ Processing chunk ${chunkNumber}/${totalChunks}...`, {
        chunkSize: chunk.length,
        linesInChunk: chunk.split('\n').length
      });

      // Build the extraction prompt for this chunk
      const isFirstChunk = i === 0;
      const extractionPrompt = buildChunkExtractionPrompt(chunk, fileName, sector, chunkNumber, totalChunks, isFirstChunk);
      
      let responseText: string;

      try {
        if (useGemini) {
          // Use Gemini 3 Pro via Replicate
          const geminiInput = {
            prompt: extractionPrompt,
            system_instruction: `Tu es un expert en analyse de fichiers et extraction de donn√©es pour le secteur "${sector}". 
Tu dois extraire TOUS les √©l√©ments (mat√©riaux, √©quipements, accessoires, articles) de ce chunk de fichier.
Tu cr√©es des cat√©gories intelligentes adapt√©es au secteur.
Tu r√©ponds UNIQUEMENT en JSON valide, sans markdown ni explication.`,
            thinking_level: "high" as const,
            temperature: 0.3,
            max_output_tokens: 16000,
          };

          const output = await replicate.run("google/gemini-3-pro", { input: geminiInput });
          responseText = Array.isArray(output) ? output.join("") : String(output);
          
        } else {
          // Fallback to OpenAI GPT-4o
          const completion = await openai.chat.completions.create({
            model: 'gpt-4o',
            messages: [
              {
                role: 'system',
                content: `Tu es un expert en analyse de fichiers et extraction de donn√©es pour le secteur "${sector}". 
Tu dois extraire TOUS les √©l√©ments de ce chunk de fichier.
Tu cr√©es des cat√©gories intelligentes adapt√©es au secteur.
Tu r√©ponds UNIQUEMENT en JSON valide.`
              },
              {
                role: 'user',
                content: extractionPrompt
              }
            ],
            temperature: 0.3,
            max_tokens: 8000,
            response_format: { type: "json_object" }
          });

          responseText = completion.choices[0]?.message?.content?.trim() || '{}';
        }

        // Parse the chunk response
        const jsonMatch = responseText.match(/\{[\s\S]*\}/);
        if (jsonMatch) {
          const chunkResult = JSON.parse(jsonMatch[0]);
          chunkResults.push({
            items: chunkResult.items || [],
            categories: chunkResult.categories || [],
          });
          
          console.log(`‚úÖ Chunk ${chunkNumber}/${totalChunks} completed:`, {
            itemsFound: chunkResult.items?.length || 0,
            categoriesFound: chunkResult.categories?.length || 0
          });
        }
      } catch (chunkError) {
        console.error(`‚ùå Error processing chunk ${chunkNumber}:`, chunkError);
        // Continue with other chunks even if one fails
      }
    }

    // Fusionner tous les r√©sultats
    const mergedResults = mergeChunkResults(chunkResults);
    const items = mergedResults.items;
    const categories = mergedResults.categories;

    console.log('üìä All chunks processed, merged results:', {
      totalItems: items.length,
      totalCategories: categories.length,
      chunksProcessed: chunkResults.length,
      totalChunks
    });

    // G√©n√©rer les suggestions d'oublis (seulement une fois, apr√®s fusion)
    let suggestions: CategorySuggestion[] = [];
    if (items.length > 0) {
      suggestions = await generateSuggestions(sector, categories, items, modelUsed);
    }

    console.log('‚úÖ Extraction completed:', {
      itemsCount: items.length,
      categoriesCount: categories.length,
      suggestionsCount: suggestions.length
    });

    // Save items to database
    if (items.length > 0) {
      const materialsToInsert = items.map(item => ({
        project_id: projectId,
        name: item.name,
        description: item.description,
        category: item.category,
        quantity: item.quantity,
        specs: {
          ...item.specs,
          unit: item.unit,
          extracted_by: modelUsed,
          sector: sector,
        },
      }));

      const { error: insertError } = await supabase
        .from('materials')
        .insert(materialsToInsert);

      if (insertError) {
        console.error('‚ùå Error inserting materials:', insertError);
        // Continue anyway to return the extraction result
      } else {
        console.log(`‚úÖ Inserted ${items.length} materials into database`);
      }
    }

    // Update project status
    await supabase
      .from('projects')
      .update({ mapping_status: 'completed' })
      .eq('id', projectId);

    return NextResponse.json({
      success: true,
      model: modelUsed,
      sector,
      items,
      categories,
      suggestions,
      statistics: {
        totalItems: items.length,
        itemsWithQuantity: items.filter(i => i.quantity !== null).length,
        categoriesCount: categories.length,
        suggestionsCount: suggestions.reduce((acc, s) => acc + s.missingItems.length, 0),
        chunksProcessed: chunkResults.length,
        totalChunks,
      },
    });

  } catch (error) {
    console.error('‚ùå API Error:', error);
    return NextResponse.json(
      { error: 'Internal server error', details: error instanceof Error ? error.message : 'Unknown error' },
      { status: 500 }
    );
  }
}

function buildExtractionPrompt(fileContent: string, fileName: string, sector: string): string {
  return `Tu es un expert en extraction de donn√©es pour le secteur "${sector}".

**FICHIER √Ä ANALYSER**: ${fileName}

**CONTENU DU FICHIER**:
\`\`\`
${fileContent.substring(0, 15000)}
\`\`\`

**TA MISSION**:

1. **EXTRAIRE TOUS LES √âL√âMENTS** (mat√©riaux, √©quipements, accessoires, articles, achats)
   - Chaque ligne avec un nom d'article = un √©l√©ment √† extraire
   - M√™me si la quantit√© est manquante, extraire l'√©l√©ment
   - S√©parer le nom court de la description d√©taill√©e

2. **CR√âER DES CAT√âGORIES INTELLIGENTES** adapt√©es au secteur "${sector}"
   - Cat√©gories claires et logiques
   - Regrouper les √©l√©ments similaires
   - Maximum 10-15 cat√©gories

3. **SUGG√âRER DES OUBLIS POTENTIELS** par cat√©gorie
   - Bas√© sur ton expertise du secteur "${sector}"
   - √âl√©ments souvent oubli√©s dans ce type de projet
   - Phrase explicative courte pour chaque suggestion

**FORMAT DE R√âPONSE JSON**:
{
  "items": [
    {
      "name": "Nom court de l'√©l√©ment",
      "description": "Description d√©taill√©e, sp√©cifications, dimensions, mod√®le...",
      "category": "Cat√©gorie assign√©e",
      "quantity": 10 ou null si non sp√©cifi√©,
      "unit": "Unit√© (pi√®ce, m¬≤, kg, lot, etc.)" ou null,
      "specs": {
        "reference": "REF-123",
        "marque": "Marque si mentionn√©e",
        "autres_infos": "..."
      }
    }
  ],
  "categories": [
    "Cat√©gorie 1",
    "Cat√©gorie 2",
    "..."
  ],
  "suggestions": [
    {
      "category": "Cat√©gorie concern√©e",
      "missingItems": [
        {
          "name": "√âl√©ment potentiellement oubli√©",
          "reason": "Phrase courte expliquant pourquoi c'est souvent n√©cessaire"
        }
      ]
    }
  ],
  "statistics": {
    "totalItems": 25,
    "itemsWithQuantity": 20,
    "categoriesCount": 5
  }
}

**EXEMPLES DE CAT√âGORIES PAR SECTEUR**:

Pour "H√¥tellerie": Literie, Salle de bain, √âclairage, Mobilier chambre, √âquipement cuisine, D√©coration, Linge de maison, √âlectrom√©nager, Signal√©tique

Pour "Restaurant": Cuisine professionnelle, Vaisselle, Mobilier salle, Bar, R√©frig√©ration, Cuisson, Pr√©paration, Hygi√®ne, D√©coration

Pour "Construction": Gros ≈ìuvre, Second ≈ìuvre, √âlectricit√©, Plomberie, Menuiserie, Peinture, Rev√™tements, Quincaillerie

Pour "Commerce": Mobilier commercial, √âclairage, Signal√©tique, Caisse, Stockage, D√©coration, S√©curit√©

**EXEMPLES DE SUGGESTIONS D'OUBLIS**:

Pour un h√¥tel:
- "Peignoirs" ‚Üí "Souvent oubli√© mais essentiel pour le confort client"
- "Porte-bagages" ‚Üí "Accessoire pratique attendu dans les chambres"
- "Coffre-fort" ‚Üí "S√©curit√© des effets personnels des clients"

Pour un restaurant:
- "Extincteur" ‚Üí "Obligatoire pour la s√©curit√© incendie"
- "Thermom√®tre alimentaire" ‚Üí "N√©cessaire pour le contr√¥le HACCP"
- "Bac √† graisse" ‚Üí "Requis par la r√©glementation"

R√âPONDS UNIQUEMENT EN JSON VALIDE.`;
}

/**
 * Prompt optimis√© pour l'extraction par chunk
 */
function buildChunkExtractionPrompt(
  chunkContent: string, 
  fileName: string, 
  sector: string, 
  chunkNumber: number, 
  totalChunks: number,
  isFirstChunk: boolean
): string {
  const chunkInfo = totalChunks > 1 
    ? `\n\n**NOTE**: Ceci est le chunk ${chunkNumber}/${totalChunks} du fichier. Extrais TOUS les √©l√©ments de ce chunk.`
    : '';

  return `Tu es un expert en extraction de donn√©es pour le secteur "${sector}".

**FICHIER**: ${fileName}${chunkInfo}

**CONTENU √Ä ANALYSER**:
\`\`\`
${chunkContent}
\`\`\`

**TA MISSION**:
Extraire TOUS les √©l√©ments (mat√©riaux, √©quipements, accessoires, articles) de ce ${totalChunks > 1 ? 'chunk' : 'fichier'}.

**R√àGLES**:
- Chaque ligne avec un nom = un √©l√©ment √† extraire
- M√™me si la quantit√© est manquante, extraire l'√©l√©ment
- S√©parer le nom court de la description d√©taill√©e
- Cr√©er des cat√©gories intelligentes adapt√©es au secteur "${sector}"

**FORMAT JSON**:
{
  "items": [
    {
      "name": "Nom court",
      "description": "Description d√©taill√©e ou null",
      "category": "Cat√©gorie",
      "quantity": 10 ou null,
      "unit": "Unit√© ou null",
      "specs": {}
    }
  ],
  "categories": ["Cat√©gorie 1", "Cat√©gorie 2"]
}

R√âPONDS UNIQUEMENT EN JSON VALIDE.`;
}

/**
 * G√©n√®re les suggestions d'oublis apr√®s l'extraction compl√®te
 */
async function generateSuggestions(
  sector: string,
  categories: string[],
  items: ExtractedItem[],
  modelUsed: string
): Promise<CategorySuggestion[]> {
  try {
    const itemNames = items.map(i => i.name).join(', ');
    
    const prompt = `Tu es un expert du secteur "${sector}".

**CAT√âGORIES EXISTANTES**: ${categories.join(', ')}

**√âL√âMENTS D√âJ√Ä LIST√âS** (r√©sum√©): ${itemNames.substring(0, 2000)}

**TA MISSION**:
Sugg√®re des √©l√©ments souvent oubli√©s dans ce type de projet, par cat√©gorie.
Base-toi sur ton expertise du secteur "${sector}".

**FORMAT JSON**:
{
  "suggestions": [
    {
      "category": "Cat√©gorie existante ou nouvelle",
      "missingItems": [
        {
          "name": "√âl√©ment oubli√©",
          "reason": "Phrase courte expliquant pourquoi c'est important"
        }
      ]
    }
  ]
}

**R√àGLES**:
- Maximum 3-5 suggestions par cat√©gorie
- Ne sugg√®re PAS d'√©l√©ments d√©j√† pr√©sents dans la liste
- Phrases explicatives courtes et percutantes
- Focus sur les oublis fr√©quents et importants

R√âPONDS UNIQUEMENT EN JSON VALIDE.`;

    let responseText: string;

    if (modelUsed === 'gemini-3-pro') {
      const geminiInput = {
        prompt,
        system_instruction: `Tu es un expert du secteur "${sector}". Tu sugg√®res des oublis potentiels bas√©s sur ton expertise. R√©ponds UNIQUEMENT en JSON valide.`,
        thinking_level: "low" as const,
        temperature: 0.5,
        max_output_tokens: 4000,
      };

      const output = await replicate.run("google/gemini-3-pro", { input: geminiInput });
      responseText = Array.isArray(output) ? output.join("") : String(output);
    } else {
      const completion = await openai.chat.completions.create({
        model: 'gpt-4o',
        messages: [
          {
            role: 'system',
            content: `Tu es un expert du secteur "${sector}". Tu sugg√®res des oublis potentiels. R√©ponds UNIQUEMENT en JSON valide.`
          },
          { role: 'user', content: prompt }
        ],
        temperature: 0.5,
        max_tokens: 2000,
        response_format: { type: "json_object" }
      });

      responseText = completion.choices[0]?.message?.content?.trim() || '{}';
    }

    const jsonMatch = responseText.match(/\{[\s\S]*\}/);
    if (jsonMatch) {
      const result = JSON.parse(jsonMatch[0]);
      return result.suggestions || [];
    }

    return [];
  } catch (error) {
    console.error('Error generating suggestions:', error);
    return [];
  }
}
